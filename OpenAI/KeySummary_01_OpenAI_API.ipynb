{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_4AOWGL7W45"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai api키 입력하세요')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.27.8"
      ],
      "metadata": {
        "id": "qXqtVETQ7xN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "import os\n",
        "import time\n",
        "import textwrap\n",
        "#textwrap >> python 표준 라이브러리 모듈\n",
        "#텍스트를 특정한 너비(width)로 잘라서 여러 줄로 나누는 데 유용"
      ],
      "metadata": {
        "id": "awrFB-RL8RhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0,\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]\n",
        "\n",
        "def get_completion_from_messages(messages, model=\"gpt-3.5-turbo\", temperature=0):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature, # this is the degree of randomness of the model's output\n",
        "    )\n",
        "\n",
        "    print(\"response from openai == \", response)\n",
        "    return response.choices[0].message[\"content\"]"
      ],
      "metadata": {
        "id": "TtXOA8rF8nvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = [\n",
        "    {'role' : 'system', 'content': '''\n",
        "    You're a helpful assistant.\n",
        "    Please answer in a lastly questioned language.\n",
        "    '''}\n",
        "]"
      ],
      "metadata": {
        "id": "LT5Dg8y3_EDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenAI Chat Completing API"
      ],
      "metadata": {
        "id": "jIq1PFv4_gjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = '''\n",
        "고구려 김유신 장군에 대해 상세히 설명해줘. 특히 한반도 대첩에서 새종대왕과 격렬하게\n",
        "충청도 사투리로 싸운 [사투리 대첩]일화에 대해서 설명해줘\n",
        "'''\n",
        "\n",
        "context.append({'role' : 'user', 'content': f'{prompt}'})\n",
        "\n",
        "ai_model = 'gpt-3.5-turbo-16k'\n",
        "\n",
        "#추출된 답변 부분\n",
        "response = get_completion_from_messages(context, model=ai_model)\n",
        "print(\"response from function: \", response)"
      ],
      "metadata": {
        "id": "BK5DIZM9_EBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# warp this text\n",
        "\n",
        "wrapper = textwrap.TextWrapper(width=70, break_long_words=False,\n",
        "                     replace_whitespace=False)\n",
        "\n",
        "#  break_long_words=False >> 긴 단어를 강제로 나누지 않음 . (있는 그대로)\n",
        "# 단어가 너비를 초과하면 다음줄로 넘어감 (\\n 기능 탑재)\n",
        "# replace_whitespace=False >> 연속된 공백문자를 하나의 공백문자로 대체하지 않음\n",
        "# 원본 텍스트의 공백을 그대로 유지\n",
        "word_list = wrapper.wrap(text=response)\n",
        "\n",
        "print('ai_model> ', ai_model)\n",
        "\n",
        "for element in word_list:\n",
        "    print(element)"
      ],
      "metadata": {
        "id": "GHSrVBZY_D5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Completion Model"
      ],
      "metadata": {
        "id": "1lBvIuRyKfIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = '''\n",
        "조선 총독부 윤석열 총독에 대해 상세히 설명해줘. 특히 샤넬 백 사건에 대해서\n",
        "백제 정청래 장군과 '광주 대첩'에 대한 일화에 대해 설명해줘\n",
        "'''\n",
        "\n",
        "context.append({'role' : 'user', 'content': f'{prompt}'})\n",
        "\n",
        "# completion model 과 chat completion 모델의 차이\n",
        "\n",
        "# ai_model = 'gpt-3.5-turbo-instruct'\n",
        "\n",
        "\n",
        "#chat completion 방식과 틀림 >> 에러 발생\n",
        "# response = get_completion_from_messages(context, model=ai_model)\n",
        "# print(\"response from function: \", response)\n",
        "\n"
      ],
      "metadata": {
        "id": "r6ZQkMttBSFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#comletion 방식에서 답변을 얻기 위한 함수\n",
        "def get_completion_from_instruction(instriction, model=\"gpt-3.5-turbo-instruct\",\n",
        "                                    temperature=0):\n",
        "    response = openai.Completion.create(\n",
        "        model=model,\n",
        "        prompt = instriction,\n",
        "        max_tokens = 2048,\n",
        "        temperature = temperature\n",
        "    )\n",
        "    print(\"response from function: \", response)\n",
        "    return response.choices[0].text\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "v07NrDi9LgpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = '''\n",
        "조선 총독부 윤석열 총독에 대해 상세히 설명해줘. 특히 샤넬 백 사건에 대해서\n",
        "백제 정청래 장군과 '광주 대첩'에 대한 일화에 대해 설명해줘\n",
        "'''\n",
        "ai_model = 'gpt-3.5-turbo-instruct'\n",
        "response = get_completion_from_instruction(prompt, model=ai_model)\n",
        "print(\"response from function: \", response)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fY2NMeFPLgnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# warp this text\n",
        "\n",
        "wrapper = textwrap.TextWrapper(width=70, break_long_words=False,\n",
        "                     replace_whitespace=False)\n",
        "\n",
        "#  break_long_words=False >> 긴 단어를 강제로 나누지 않음 . (있는 그대로)\n",
        "# 단어가 너비를 초과하면 다음줄로 넘어감 (\\n 기능 탑재)\n",
        "# replace_whitespace=False >> 연속된 공백문자를 하나의 공백문자로 대체하지 않음\n",
        "# 원본 텍스트의 공백을 그대로 유지\n",
        "word_list = wrapper.wrap(text=response)\n",
        "\n",
        "print('ai_model> ', ai_model)\n",
        "\n",
        "for element in word_list:\n",
        "    print(element)"
      ],
      "metadata": {
        "id": "zF_jIiQ2M3fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#랭체인 사용해서 호출해보기\n",
        "\n",
        "!pip install langchain\n",
        "!pip install langchain_community"
      ],
      "metadata": {
        "id": "zCjl4B1lNxq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n"
      ],
      "metadata": {
        "id": "bfm52FfKOFsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ai_model = 'gpt-4o'\n",
        "\n",
        "model = ChatOpenAI(model=ai_model, temperature=0)"
      ],
      "metadata": {
        "id": "XNdz6ywDOFqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = '''\n",
        "고구려 김유신 장군에 대해 파악해줘\n",
        "'''\n",
        "\n",
        "response = model.invoke(prompt)\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "J5L1vN0wO6po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 답변 text 확인\n",
        "\n",
        "output = response.content\n",
        "\n",
        "wrapper = textwrap.TextWrapper(width=50, break_long_words=False,\n",
        "                     replace_whitespace=False)\n",
        "\n",
        "word_list = wrapper.wrap(text=output)\n",
        "print('ai_model >', ai_model)\n",
        "\n",
        "for element in word_list:\n",
        "    print(element)"
      ],
      "metadata": {
        "id": "wdSAqGosPHl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chain 사용해보기"
      ],
      "metadata": {
        "id": "Lyoj2RW0PvqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import LLMChain, PromptTemplate"
      ],
      "metadata": {
        "id": "oqliRG3APvfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "아래의 질문에 대해서 경상도 사투리로 대답해줘\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template = template, input_variables=['question'])\n",
        "print('prompt template >', prompt)\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm = model)\n",
        "print('llm chain >', llm_chain)"
      ],
      "metadata": {
        "id": "yZxdkSZwPvbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_question = '''\n",
        "백제 영웅인 강감찬 장군에 대해 파악해줘\n",
        "'''\n",
        "\n",
        "response = llm_chain.run(question= my_question)\n",
        "response"
      ],
      "metadata": {
        "id": "Kb-uKq2_RQwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "wrapper = textwrap.TextWrapper(width=50, break_long_words=False,\n",
        "                     replace_whitespace=False)\n",
        "\n",
        "word_list = wrapper.wrap(text=response)\n",
        "print('ai_model >', ai_model)\n",
        "\n",
        "for element in word_list:\n",
        "    print(element)"
      ],
      "metadata": {
        "id": "QoYJjwiqPvY3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}